{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TASK-2 : ru_en.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fa48397019e141a1b8775d5b676cc462":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_174716257c3e42a4aae22f925e72b452","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4035af35dc76415386dd413d698342d1","IPY_MODEL_d847b577694f4b56b7b353d249068310"]}},"174716257c3e42a4aae22f925e72b452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4035af35dc76415386dd413d698342d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_49dc1baf8a6d4a17ad1738d798514d6b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":918311367,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":918311367,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eec85356f4cd4d978f1ab992ba39d336"}},"d847b577694f4b56b7b353d249068310":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_08aa7a3ccdfc4ceeba1543d60b841aba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 918M/918M [01:19&lt;00:00, 11.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eea2c0423617449da77054c27b883d7d"}},"49dc1baf8a6d4a17ad1738d798514d6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"eec85356f4cd4d978f1ab992ba39d336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08aa7a3ccdfc4ceeba1543d60b841aba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eea2c0423617449da77054c27b883d7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83a81e3f12aa40df90bb4bd75b581363":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c214bf7a8e95488c9d084b0fd96c5d2f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_836ea032483b4fbf8d598f116c8946cc","IPY_MODEL_55524e22bff5487ea5682a36ab20940e"]}},"c214bf7a8e95488c9d084b0fd96c5d2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"836ea032483b4fbf8d598f116c8946cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b17bf745f6d74cb59c376f4b271a4b1f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":75178032,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":75178032,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_945aa93baf56449b89f6b91fdc877530"}},"55524e22bff5487ea5682a36ab20940e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_988ccd5026d0400c8b2b65e6a27e86d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 75.2M/75.2M [01:00&lt;00:00, 1.25MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cbe2b0770eca4ab4b68eb51443199b11"}},"b17bf745f6d74cb59c376f4b271a4b1f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"945aa93baf56449b89f6b91fdc877530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"988ccd5026d0400c8b2b65e6a27e86d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cbe2b0770eca4ab4b68eb51443199b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e457d3b8b03b47f7a19a11dfc8cc3131":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_905e94f4652648ec9f759347f3ec54fe","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0f17dbc7c5804bd5b19ca1abb847e0f2","IPY_MODEL_5e260a70867745b3a21b89883bd5158d"]}},"905e94f4652648ec9f759347f3ec54fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f17dbc7c5804bd5b19ca1abb847e0f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_96e090f820c249fb851be1b8203174e9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":9487481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9487481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b59c385f4f6d4937a16a2d0b40640eba"}},"5e260a70867745b3a21b89883bd5158d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0dcd96178554481869db4704eab5c23","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9.49M/9.49M [00:00&lt;00:00, 10.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_02991c280a0b4f999d416230178c18de"}},"96e090f820c249fb851be1b8203174e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b59c385f4f6d4937a16a2d0b40640eba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0dcd96178554481869db4704eab5c23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"02991c280a0b4f999d416230178c18de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"41f052ec2b6f4627ac60875b824d74f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_25ed74b68c5c4cae9b9231de0f4bf3d8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ed0f09249b904d44b4f4d8bfa17696c5","IPY_MODEL_356cc18dea654c1db9fc86b803b644aa"]}},"25ed74b68c5c4cae9b9231de0f4bf3d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ed0f09249b904d44b4f4d8bfa17696c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_03b721b2e7794eca82545cfd30f8a7ae","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":38654961,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":38654961,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a6dd977fd5b486e9bf6128624f92bf5"}},"356cc18dea654c1db9fc86b803b644aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2be4b14422fa468b816ae00e70a70707","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 38.7M/38.7M [00:57&lt;00:00, 672kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39bbca7c63594c84b853a910cdac91a6"}},"03b721b2e7794eca82545cfd30f8a7ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8a6dd977fd5b486e9bf6128624f92bf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2be4b14422fa468b816ae00e70a70707":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"39bbca7c63594c84b853a910cdac91a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c70bd4cd65c24978b6ecafbed6e986bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7d5b2e8b50b74be284f2010bb6b5ee8c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8afadc0130074d6f876c40d7c8d9d82d","IPY_MODEL_eff8b5e9434e4f549368016a1812a235"]}},"7d5b2e8b50b74be284f2010bb6b5ee8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8afadc0130074d6f876c40d7c8d9d82d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_137ee6bdf413434e9644a411afbaa048","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a6ee1f4730e24428aee283bcb50dae9a"}},"eff8b5e9434e4f549368016a1812a235":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8612ca3ca54e486e994c49ceddbecf9c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1516162/0 [00:39&lt;00:00, 53498.25 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60ab704e0c4c4f518b3f5b0ddbccc412"}},"137ee6bdf413434e9644a411afbaa048":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a6ee1f4730e24428aee283bcb50dae9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8612ca3ca54e486e994c49ceddbecf9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"60ab704e0c4c4f518b3f5b0ddbccc412":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c10e62a211b49dd92fb1bdd2f5a3747":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e1a212df22584de89680c8e384bbfe33","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9b0f9c477da64bb6b5b0884d3a3b27b7","IPY_MODEL_ada0ba0076bb41dfa8aa63c2c9530680"]}},"e1a212df22584de89680c8e384bbfe33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b0f9c477da64bb6b5b0884d3a3b27b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5deac2194d8946f0ad8ba9769c6f4ea7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d5ceed8f54a949869983b2a226916f0a"}},"ada0ba0076bb41dfa8aa63c2c9530680":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be92d0f147ba482899d513fbc2ce9ed9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2818/0 [00:00&lt;00:00, 23397.46 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_796ab2ad2b0c4d90a2f05c37b54d37b8"}},"5deac2194d8946f0ad8ba9769c6f4ea7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d5ceed8f54a949869983b2a226916f0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be92d0f147ba482899d513fbc2ce9ed9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"796ab2ad2b0c4d90a2f05c37b54d37b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"06dc47c785f64a1ca9bcc26e34f4fc81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_30cd18408b4443e6ac8c0121b0cfc316","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_54ff6b557620459b900d6b8c4e360b04","IPY_MODEL_f43c481c49af4af593a684e3a2d3cea2"]}},"30cd18408b4443e6ac8c0121b0cfc316":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"54ff6b557620459b900d6b8c4e360b04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9268b412bf7f4c3da298641415be5cbc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9f4f778ceac543d588f8b44b095915a3"}},"f43c481c49af4af593a684e3a2d3cea2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c109d439ea8a48b194a550ffa8ca2218","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2998/0 [00:00&lt;00:00, 20019.59 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_281961f6fa8b455cba2de87b1cc158c5"}},"9268b412bf7f4c3da298641415be5cbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9f4f778ceac543d588f8b44b095915a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c109d439ea8a48b194a550ffa8ca2218":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"281961f6fa8b455cba2de87b1cc158c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f91567e7f9747cd85c66fdbadee4cbf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_11a64dbae4fc4916b730a00273d46bc3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_438585cb935e4563b545780de446d025","IPY_MODEL_2f2e34efb1a84d3f8e9691e9b520f887"]}},"11a64dbae4fc4916b730a00273d46bc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"438585cb935e4563b545780de446d025":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6b41302bd3f0460a9d0deea89671aeb0","_dom_classes":[],"description":"Downloading: ","_model_name":"FloatProgressModel","bar_style":"success","max":2236,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2236,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_992c6e80cf2d485bb683038052fde408"}},"2f2e34efb1a84d3f8e9691e9b520f887":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_80807720ce2b4a60816aa0c8404636f6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.40k/? [00:00&lt;00:00, 72.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14f353c64ae6419c8b91cd4601ab4734"}},"6b41302bd3f0460a9d0deea89671aeb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"992c6e80cf2d485bb683038052fde408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"80807720ce2b4a60816aa0c8404636f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14f353c64ae6419c8b91cd4601ab4734":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67a020f1972446328fdf7fff9f51b0c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_828f82909e4f49b089b6ebe8f1e25d26","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9e9df1d8c0664c7395043a52563a46a8","IPY_MODEL_d1753322848642e489118ffd6c623fa0"]}},"828f82909e4f49b089b6ebe8f1e25d26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e9df1d8c0664c7395043a52563a46a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_19cd9632b8734918973a91db308a9112","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":42,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_86f2de8c7d6e42f78c0e40be2eaa1cad"}},"d1753322848642e489118ffd6c623fa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ad25bd89706641cd9c3c99cde6fbfc2d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 42.0/42.0 [00:00&lt;00:00, 388B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_53381952294f4517ab05793bb1e93b9e"}},"19cd9632b8734918973a91db308a9112":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"86f2de8c7d6e42f78c0e40be2eaa1cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad25bd89706641cd9c3c99cde6fbfc2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"53381952294f4517ab05793bb1e93b9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b20f15a1a6b49299cb11c722f5461c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_983456d3ac8d46be90bce634b6563175","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c37bda362a834013a17299a2d3f0bfdc","IPY_MODEL_2e5a076bc34c44fcba785cf3deb4fc89"]}},"983456d3ac8d46be90bce634b6563175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c37bda362a834013a17299a2d3f0bfdc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9b908d188040497f8666b10a509c06cc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e8c401fac668484ca1141bed921f32e3"}},"2e5a076bc34c44fcba785cf3deb4fc89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d308b0c6e32c4f2ea91e2786cf52c063","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.13k/1.13k [00:00&lt;00:00, 1.42kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3010db1f965843b3828f45e2068b84fb"}},"9b908d188040497f8666b10a509c06cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e8c401fac668484ca1141bed921f32e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d308b0c6e32c4f2ea91e2786cf52c063":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3010db1f965843b3828f45e2068b84fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d7cf2d4a6cd041e58e615db269a384f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_de93e512ba3d430abaf16554e31fb6f2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8739ae831dbf470a8b05381aecd9aa7c","IPY_MODEL_19fe5ccd7b7a4edab0e3ba8c0fc82d8f"]}},"de93e512ba3d430abaf16554e31fb6f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8739ae831dbf470a8b05381aecd9aa7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_18c263596e6b48c09e334b2122392032","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1080169,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1080169,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_91b92dacac0940b9bbe7d6e01753aaf5"}},"19fe5ccd7b7a4edab0e3ba8c0fc82d8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_72cd1cb281dd486b8bc732ed2fc7fd71","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.08M/1.08M [00:00&lt;00:00, 3.75MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a758faf936994338b10e9e9c73a6afe7"}},"18c263596e6b48c09e334b2122392032":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"91b92dacac0940b9bbe7d6e01753aaf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72cd1cb281dd486b8bc732ed2fc7fd71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a758faf936994338b10e9e9c73a6afe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84ea16a9af0e41729d68450ecf95eb57":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6b73d0d6ba414376821d63338d7d4774","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_181ee974fc5d46c8a93131365abec48c","IPY_MODEL_563b92bb7ab24399a000be9840168ad7"]}},"6b73d0d6ba414376821d63338d7d4774":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"181ee974fc5d46c8a93131365abec48c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_78af3205ca224272b07dcca18b606454","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":802781,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":802781,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_be2bcab8ca8349ccb54809cc943e6731"}},"563b92bb7ab24399a000be9840168ad7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3fe8e7f346cd4a5e8dadd6a87f8c6b1f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 803k/803k [00:00&lt;00:00, 6.05MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11e88be849d342e0bfb4261d3c130339"}},"78af3205ca224272b07dcca18b606454":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"be2bcab8ca8349ccb54809cc943e6731":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3fe8e7f346cd4a5e8dadd6a87f8c6b1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"11e88be849d342e0bfb4261d3c130339":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07e1f49ac49e47c9b4a4585e4c05abfa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9c081aa48fe74a778e5a28025329bf6c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dac5cfd543b04037819218e97882b13e","IPY_MODEL_d9de43f1bf3f4c0a8856b472835b15b9"]}},"9c081aa48fe74a778e5a28025329bf6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dac5cfd543b04037819218e97882b13e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_91dc575f4b8540488a268a44fd8351ac","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":2601758,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2601758,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f79fb2700f44456b1a7dbc6f24a4eb7"}},"d9de43f1bf3f4c0a8856b472835b15b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_83d881c6c2ba407eaebed61b59a178e6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.60M/2.60M [00:00&lt;00:00, 7.10MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b98bd7258a474229bc15c2ecf20765fb"}},"91dc575f4b8540488a268a44fd8351ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7f79fb2700f44456b1a7dbc6f24a4eb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83d881c6c2ba407eaebed61b59a178e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b98bd7258a474229bc15c2ecf20765fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7bb410c0c4a401882ceb9a319628a18":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_72e3b7ea07064351a7ac70ecaf4fe95b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_624dfd0f93074fe0a56cfb7325a36680","IPY_MODEL_67937f4f992545ad8508aa5640f912fa"]}},"72e3b7ea07064351a7ac70ecaf4fe95b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"624dfd0f93074fe0a56cfb7325a36680":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c477a3b5e76e454eb00810374340d3e8","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1517,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1517,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_240e640984644497a69be6179daabca8"}},"67937f4f992545ad8508aa5640f912fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e09be5c6bfc74e0db4ec3024108d5ada","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1517/1517 [1:56:12&lt;00:00,  4.60s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_808285db09d044fea17ee01d52106e99"}},"c477a3b5e76e454eb00810374340d3e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"240e640984644497a69be6179daabca8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e09be5c6bfc74e0db4ec3024108d5ada":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"808285db09d044fea17ee01d52106e99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7cf496c10bd64171aeb147f7004299e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_00abc36bae184aacab2e47eae3f42b34","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e106000d5f1941f28fe501786d81b972","IPY_MODEL_83de708327d94293bc439f25f408d532"]}},"00abc36bae184aacab2e47eae3f42b34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e106000d5f1941f28fe501786d81b972":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1654cf42b1054804ab8ba769dbea885a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cfbdb97580b04e419ae66eec233a4b6b"}},"83de708327d94293bc439f25f408d532":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_191b596525e345fa998bd64e4086ecbd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/3 [00:02&lt;00:00,  1.31ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_132fe2c318b94854879fad591ae5e088"}},"1654cf42b1054804ab8ba769dbea885a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cfbdb97580b04e419ae66eec233a4b6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"191b596525e345fa998bd64e4086ecbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"132fe2c318b94854879fad591ae5e088":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6898ed384bcc443b97b3e8b094ea410c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9202631aaabf487b85ae5746401b263c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cb1318332a8a4819b6efb0135be3ecc4","IPY_MODEL_65e8d015bd7449d3af17328e7ce9b162"]}},"9202631aaabf487b85ae5746401b263c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb1318332a8a4819b6efb0135be3ecc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9e518a5a0c754a8f9cafe0280e9e78be","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2905e13fd95d4b14b7dd594636767535"}},"65e8d015bd7449d3af17328e7ce9b162":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e034e57cc7d74feaa9c52a897fc47768","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/3 [00:01&lt;00:00,  2.66ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dce9f2674075432ea48058fe0ddaf4da"}},"9e518a5a0c754a8f9cafe0280e9e78be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2905e13fd95d4b14b7dd594636767535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e034e57cc7d74feaa9c52a897fc47768":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dce9f2674075432ea48058fe0ddaf4da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d34eca4908e4155b5c4fa941d291832":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_825a74b1706a4456abdfe1f3d88acafc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_61b36bb646d6419a8a34fdc72480de19","IPY_MODEL_e2a329897d9c4e94ac22dac95621e837"]}},"825a74b1706a4456abdfe1f3d88acafc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61b36bb646d6419a8a34fdc72480de19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_de1482c26e0b43c2ae6f9c56db6367d1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":306991893,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":306991893,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df7db053bf294a269641edf622b6d512"}},"e2a329897d9c4e94ac22dac95621e837":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d8bc3089789b4a959569bf7313171816","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 307M/307M [00:22&lt;00:00, 13.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a6049588ad14619a1a6c140a7ff58af"}},"de1482c26e0b43c2ae6f9c56db6367d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"df7db053bf294a269641edf622b6d512":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d8bc3089789b4a959569bf7313171816":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7a6049588ad14619a1a6c140a7ff58af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-eJU3lkY1z84","executionInfo":{"status":"ok","timestamp":1625401047707,"user_tz":-330,"elapsed":21938,"user":{"displayName":"Siddartha Chennareddy","photoUrl":"","userId":"05578587641916586628"}},"outputId":"4c49808f-94b8-460a-e5c2-59b4d679c885"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fa48397019e141a1b8775d5b676cc462","174716257c3e42a4aae22f925e72b452","4035af35dc76415386dd413d698342d1","d847b577694f4b56b7b353d249068310","49dc1baf8a6d4a17ad1738d798514d6b","eec85356f4cd4d978f1ab992ba39d336","08aa7a3ccdfc4ceeba1543d60b841aba","eea2c0423617449da77054c27b883d7d","83a81e3f12aa40df90bb4bd75b581363","c214bf7a8e95488c9d084b0fd96c5d2f","836ea032483b4fbf8d598f116c8946cc","55524e22bff5487ea5682a36ab20940e","b17bf745f6d74cb59c376f4b271a4b1f","945aa93baf56449b89f6b91fdc877530","988ccd5026d0400c8b2b65e6a27e86d4","cbe2b0770eca4ab4b68eb51443199b11","e457d3b8b03b47f7a19a11dfc8cc3131","905e94f4652648ec9f759347f3ec54fe","0f17dbc7c5804bd5b19ca1abb847e0f2","5e260a70867745b3a21b89883bd5158d","96e090f820c249fb851be1b8203174e9","b59c385f4f6d4937a16a2d0b40640eba","b0dcd96178554481869db4704eab5c23","02991c280a0b4f999d416230178c18de","41f052ec2b6f4627ac60875b824d74f3","25ed74b68c5c4cae9b9231de0f4bf3d8","ed0f09249b904d44b4f4d8bfa17696c5","356cc18dea654c1db9fc86b803b644aa","03b721b2e7794eca82545cfd30f8a7ae","8a6dd977fd5b486e9bf6128624f92bf5","2be4b14422fa468b816ae00e70a70707","39bbca7c63594c84b853a910cdac91a6","c70bd4cd65c24978b6ecafbed6e986bf","7d5b2e8b50b74be284f2010bb6b5ee8c","8afadc0130074d6f876c40d7c8d9d82d","eff8b5e9434e4f549368016a1812a235","137ee6bdf413434e9644a411afbaa048","a6ee1f4730e24428aee283bcb50dae9a","8612ca3ca54e486e994c49ceddbecf9c","60ab704e0c4c4f518b3f5b0ddbccc412","6c10e62a211b49dd92fb1bdd2f5a3747","e1a212df22584de89680c8e384bbfe33","9b0f9c477da64bb6b5b0884d3a3b27b7","ada0ba0076bb41dfa8aa63c2c9530680","5deac2194d8946f0ad8ba9769c6f4ea7","d5ceed8f54a949869983b2a226916f0a","be92d0f147ba482899d513fbc2ce9ed9","796ab2ad2b0c4d90a2f05c37b54d37b8","06dc47c785f64a1ca9bcc26e34f4fc81","30cd18408b4443e6ac8c0121b0cfc316","54ff6b557620459b900d6b8c4e360b04","f43c481c49af4af593a684e3a2d3cea2","9268b412bf7f4c3da298641415be5cbc","9f4f778ceac543d588f8b44b095915a3","c109d439ea8a48b194a550ffa8ca2218","281961f6fa8b455cba2de87b1cc158c5","4f91567e7f9747cd85c66fdbadee4cbf","11a64dbae4fc4916b730a00273d46bc3","438585cb935e4563b545780de446d025","2f2e34efb1a84d3f8e9691e9b520f887","6b41302bd3f0460a9d0deea89671aeb0","992c6e80cf2d485bb683038052fde408","80807720ce2b4a60816aa0c8404636f6","14f353c64ae6419c8b91cd4601ab4734","67a020f1972446328fdf7fff9f51b0c7","828f82909e4f49b089b6ebe8f1e25d26","9e9df1d8c0664c7395043a52563a46a8","d1753322848642e489118ffd6c623fa0","19cd9632b8734918973a91db308a9112","86f2de8c7d6e42f78c0e40be2eaa1cad","ad25bd89706641cd9c3c99cde6fbfc2d","53381952294f4517ab05793bb1e93b9e","7b20f15a1a6b49299cb11c722f5461c4","983456d3ac8d46be90bce634b6563175","c37bda362a834013a17299a2d3f0bfdc","2e5a076bc34c44fcba785cf3deb4fc89","9b908d188040497f8666b10a509c06cc","e8c401fac668484ca1141bed921f32e3","d308b0c6e32c4f2ea91e2786cf52c063","3010db1f965843b3828f45e2068b84fb","d7cf2d4a6cd041e58e615db269a384f2","de93e512ba3d430abaf16554e31fb6f2","8739ae831dbf470a8b05381aecd9aa7c","19fe5ccd7b7a4edab0e3ba8c0fc82d8f","18c263596e6b48c09e334b2122392032","91b92dacac0940b9bbe7d6e01753aaf5","72cd1cb281dd486b8bc732ed2fc7fd71","a758faf936994338b10e9e9c73a6afe7","84ea16a9af0e41729d68450ecf95eb57","6b73d0d6ba414376821d63338d7d4774","181ee974fc5d46c8a93131365abec48c","563b92bb7ab24399a000be9840168ad7","78af3205ca224272b07dcca18b606454","be2bcab8ca8349ccb54809cc943e6731","3fe8e7f346cd4a5e8dadd6a87f8c6b1f","11e88be849d342e0bfb4261d3c130339","07e1f49ac49e47c9b4a4585e4c05abfa","9c081aa48fe74a778e5a28025329bf6c","dac5cfd543b04037819218e97882b13e","d9de43f1bf3f4c0a8856b472835b15b9","91dc575f4b8540488a268a44fd8351ac","7f79fb2700f44456b1a7dbc6f24a4eb7","83d881c6c2ba407eaebed61b59a178e6","b98bd7258a474229bc15c2ecf20765fb","f7bb410c0c4a401882ceb9a319628a18","72e3b7ea07064351a7ac70ecaf4fe95b","624dfd0f93074fe0a56cfb7325a36680","67937f4f992545ad8508aa5640f912fa","c477a3b5e76e454eb00810374340d3e8","240e640984644497a69be6179daabca8","e09be5c6bfc74e0db4ec3024108d5ada","808285db09d044fea17ee01d52106e99","7cf496c10bd64171aeb147f7004299e1","00abc36bae184aacab2e47eae3f42b34","e106000d5f1941f28fe501786d81b972","83de708327d94293bc439f25f408d532","1654cf42b1054804ab8ba769dbea885a","cfbdb97580b04e419ae66eec233a4b6b","191b596525e345fa998bd64e4086ecbd","132fe2c318b94854879fad591ae5e088","6898ed384bcc443b97b3e8b094ea410c","9202631aaabf487b85ae5746401b263c","cb1318332a8a4819b6efb0135be3ecc4","65e8d015bd7449d3af17328e7ce9b162","9e518a5a0c754a8f9cafe0280e9e78be","2905e13fd95d4b14b7dd594636767535","e034e57cc7d74feaa9c52a897fc47768","dce9f2674075432ea48058fe0ddaf4da","1d34eca4908e4155b5c4fa941d291832","825a74b1706a4456abdfe1f3d88acafc","61b36bb646d6419a8a34fdc72480de19","e2a329897d9c4e94ac22dac95621e837","de1482c26e0b43c2ae6f9c56db6367d1","df7db053bf294a269641edf622b6d512","d8bc3089789b4a959569bf7313171816","7a6049588ad14619a1a6c140a7ff58af"]},"id":"i51EWNb82Fvv","executionInfo":{"status":"ok","timestamp":1625417898061,"user_tz":-330,"elapsed":16848024,"user":{"displayName":"Siddartha Chennareddy","photoUrl":"","userId":"05578587641916586628"}},"outputId":"def1bf56-675b-491f-8b1d-10705a3b407e"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","\n","! pip install datasets transformers sacrebleu\n","\n","model_checkpoint = \"Helsinki-NLP/opus-mt-ru-en\"\n","\n","from datasets import load_dataset, load_metric\n","\n","raw_datasets = load_dataset(\"wmt16\", \"ru-en\")\n","metric = load_metric(\"sacrebleu\")\n","\n","! pip install sentencepiece\n","\n","from transformers import AutoTokenizer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","if \"mbart\" in model_checkpoint:\n","    tokenizer.src_lang = \"ru-RU\"\n","    tokenizer.tgt_lang = \"en-XX\"\n","\n","if model_checkpoint in [ \"t5-base\"]:\n","    prefix = \"translate Russian to English: \"\n","else:\n","    prefix = \"\"\n","\n","max_input_length = 128\n","max_target_length = 128\n","source_lang = \"ru\"\n","target_lang = \"en\"\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n","    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","\n","batch_size = 16\n","args = Seq2SeqTrainingArguments(\n","    \"test-translation\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=10e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=1,\n","    predict_with_generate=True,\n","    fp16=True,\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result\n","\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.8.0)\n","Collecting transformers\n","  Using cached https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl\n","Collecting sacrebleu\n","  Using cached https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.6.1)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.13)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Collecting portalocker==2.0.0\n","  Using cached https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","\u001b[31mERROR: transformers 4.8.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.13 which is incompatible.\u001b[0m\n","Installing collected packages: transformers, portalocker, sacrebleu\n","Successfully installed portalocker-2.0.0 sacrebleu-1.5.1 transformers-4.8.2\n","Downloading and preparing dataset wmt16/ru-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ru-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa48397019e141a1b8775d5b676cc462","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=918311367.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83a81e3f12aa40df90bb4bd75b581363","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=75178032.0, style=ProgressStyle(descrip…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e457d3b8b03b47f7a19a11dfc8cc3131","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9487481.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41f052ec2b6f4627ac60875b824d74f3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=38654961.0, style=ProgressStyle(descrip…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c70bd4cd65c24978b6ecafbed6e986bf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c10e62a211b49dd92fb1bdd2f5a3747","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06dc47c785f64a1ca9bcc26e34f4fc81","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Dataset wmt16 downloaded and prepared to /root/.cache/huggingface/datasets/wmt16/ru-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a. Subsequent calls will reuse this data.\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f91567e7f9747cd85c66fdbadee4cbf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2236.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 29.1MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67a020f1972446328fdf7fff9f51b0c7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=42.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b20f15a1a6b49299cb11c722f5461c4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1133.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7cf2d4a6cd041e58e615db269a384f2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1080169.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84ea16a9af0e41729d68450ecf95eb57","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=802781.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07e1f49ac49e47c9b4a4585e4c05abfa","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2601758.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7bb410c0c4a401882ceb9a319628a18","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1517.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cf496c10bd64171aeb147f7004299e1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6898ed384bcc443b97b3e8b094ea410c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d34eca4908e4155b5c4fa941d291832","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=306991893.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Using amp fp16 backend\n","The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n","***** Running training *****\n","  Num examples = 1516162\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 94761\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='94761' max='94761' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94761/94761 4:33:31, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.175800</td>\n","      <td>1.504680</td>\n","      <td>29.015500</td>\n","      <td>26.467400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-500\n","Configuration saved in test-translation/checkpoint-500/config.json\n","Model weights saved in test-translation/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to test-translation/checkpoint-1000\n","Configuration saved in test-translation/checkpoint-1000/config.json\n","Model weights saved in test-translation/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to test-translation/checkpoint-1500\n","Configuration saved in test-translation/checkpoint-1500/config.json\n","Model weights saved in test-translation/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-1500/special_tokens_map.json\n","Saving model checkpoint to test-translation/checkpoint-2000\n","Configuration saved in test-translation/checkpoint-2000/config.json\n","Model weights saved in test-translation/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-2500\n","Configuration saved in test-translation/checkpoint-2500/config.json\n","Model weights saved in test-translation/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-1000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-3000\n","Configuration saved in test-translation/checkpoint-3000/config.json\n","Model weights saved in test-translation/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-3500\n","Configuration saved in test-translation/checkpoint-3500/config.json\n","Model weights saved in test-translation/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-4000\n","Configuration saved in test-translation/checkpoint-4000/config.json\n","Model weights saved in test-translation/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-2500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-4500\n","Configuration saved in test-translation/checkpoint-4500/config.json\n","Model weights saved in test-translation/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-3000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-5000\n","Configuration saved in test-translation/checkpoint-5000/config.json\n","Model weights saved in test-translation/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-5500\n","Configuration saved in test-translation/checkpoint-5500/config.json\n","Model weights saved in test-translation/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-6000\n","Configuration saved in test-translation/checkpoint-6000/config.json\n","Model weights saved in test-translation/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-6000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-6500\n","Configuration saved in test-translation/checkpoint-6500/config.json\n","Model weights saved in test-translation/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-6500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-5000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-7000\n","Configuration saved in test-translation/checkpoint-7000/config.json\n","Model weights saved in test-translation/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-7000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-5500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-7500\n","Configuration saved in test-translation/checkpoint-7500/config.json\n","Model weights saved in test-translation/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-7500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-6000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-8000\n","Configuration saved in test-translation/checkpoint-8000/config.json\n","Model weights saved in test-translation/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-8000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-6500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-8500\n","Configuration saved in test-translation/checkpoint-8500/config.json\n","Model weights saved in test-translation/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-8500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-7000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-9000\n","Configuration saved in test-translation/checkpoint-9000/config.json\n","Model weights saved in test-translation/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-9000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-7500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-9500\n","Configuration saved in test-translation/checkpoint-9500/config.json\n","Model weights saved in test-translation/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-9500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-8000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-10000\n","Configuration saved in test-translation/checkpoint-10000/config.json\n","Model weights saved in test-translation/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-10000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-8500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-10500\n","Configuration saved in test-translation/checkpoint-10500/config.json\n","Model weights saved in test-translation/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-10500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-9000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-11000\n","Configuration saved in test-translation/checkpoint-11000/config.json\n","Model weights saved in test-translation/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-11000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-9500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-11500\n","Configuration saved in test-translation/checkpoint-11500/config.json\n","Model weights saved in test-translation/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-11500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-10000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-12000\n","Configuration saved in test-translation/checkpoint-12000/config.json\n","Model weights saved in test-translation/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-12000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-10500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-12500\n","Configuration saved in test-translation/checkpoint-12500/config.json\n","Model weights saved in test-translation/checkpoint-12500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-12500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-12500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-11000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-13000\n","Configuration saved in test-translation/checkpoint-13000/config.json\n","Model weights saved in test-translation/checkpoint-13000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-13000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-13000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-11500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-13500\n","Configuration saved in test-translation/checkpoint-13500/config.json\n","Model weights saved in test-translation/checkpoint-13500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-13500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-13500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-12000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-14000\n","Configuration saved in test-translation/checkpoint-14000/config.json\n","Model weights saved in test-translation/checkpoint-14000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-14000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-14000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-12500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-14500\n","Configuration saved in test-translation/checkpoint-14500/config.json\n","Model weights saved in test-translation/checkpoint-14500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-14500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-14500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-13000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-15000\n","Configuration saved in test-translation/checkpoint-15000/config.json\n","Model weights saved in test-translation/checkpoint-15000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-15000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-15000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-13500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-15500\n","Configuration saved in test-translation/checkpoint-15500/config.json\n","Model weights saved in test-translation/checkpoint-15500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-15500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-15500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-14000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-16000\n","Configuration saved in test-translation/checkpoint-16000/config.json\n","Model weights saved in test-translation/checkpoint-16000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-16000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-16000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-14500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-16500\n","Configuration saved in test-translation/checkpoint-16500/config.json\n","Model weights saved in test-translation/checkpoint-16500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-16500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-16500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-15000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-17000\n","Configuration saved in test-translation/checkpoint-17000/config.json\n","Model weights saved in test-translation/checkpoint-17000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-17000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-17000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-15500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-17500\n","Configuration saved in test-translation/checkpoint-17500/config.json\n","Model weights saved in test-translation/checkpoint-17500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-17500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-17500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-16000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-18000\n","Configuration saved in test-translation/checkpoint-18000/config.json\n","Model weights saved in test-translation/checkpoint-18000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-18000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-18000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-16500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-18500\n","Configuration saved in test-translation/checkpoint-18500/config.json\n","Model weights saved in test-translation/checkpoint-18500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-18500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-18500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-17000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-19000\n","Configuration saved in test-translation/checkpoint-19000/config.json\n","Model weights saved in test-translation/checkpoint-19000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-19000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-19000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-17500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-19500\n","Configuration saved in test-translation/checkpoint-19500/config.json\n","Model weights saved in test-translation/checkpoint-19500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-19500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-19500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-18000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-20000\n","Configuration saved in test-translation/checkpoint-20000/config.json\n","Model weights saved in test-translation/checkpoint-20000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-20000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-20000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-18500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-20500\n","Configuration saved in test-translation/checkpoint-20500/config.json\n","Model weights saved in test-translation/checkpoint-20500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-20500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-20500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-19000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-21000\n","Configuration saved in test-translation/checkpoint-21000/config.json\n","Model weights saved in test-translation/checkpoint-21000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-21000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-21000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-19500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-21500\n","Configuration saved in test-translation/checkpoint-21500/config.json\n","Model weights saved in test-translation/checkpoint-21500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-21500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-21500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-20000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-22000\n","Configuration saved in test-translation/checkpoint-22000/config.json\n","Model weights saved in test-translation/checkpoint-22000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-22000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-22000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-20500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-22500\n","Configuration saved in test-translation/checkpoint-22500/config.json\n","Model weights saved in test-translation/checkpoint-22500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-22500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-22500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-21000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-23000\n","Configuration saved in test-translation/checkpoint-23000/config.json\n","Model weights saved in test-translation/checkpoint-23000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-23000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-23000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-21500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-23500\n","Configuration saved in test-translation/checkpoint-23500/config.json\n","Model weights saved in test-translation/checkpoint-23500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-23500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-23500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-22000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-24000\n","Configuration saved in test-translation/checkpoint-24000/config.json\n","Model weights saved in test-translation/checkpoint-24000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-24000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-24000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-22500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-24500\n","Configuration saved in test-translation/checkpoint-24500/config.json\n","Model weights saved in test-translation/checkpoint-24500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-24500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-24500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-23000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-25000\n","Configuration saved in test-translation/checkpoint-25000/config.json\n","Model weights saved in test-translation/checkpoint-25000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-25000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-25000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-23500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-25500\n","Configuration saved in test-translation/checkpoint-25500/config.json\n","Model weights saved in test-translation/checkpoint-25500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-25500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-25500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-24000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-26000\n","Configuration saved in test-translation/checkpoint-26000/config.json\n","Model weights saved in test-translation/checkpoint-26000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-26000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-26000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-24500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-26500\n","Configuration saved in test-translation/checkpoint-26500/config.json\n","Model weights saved in test-translation/checkpoint-26500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-26500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-26500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-25000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-27000\n","Configuration saved in test-translation/checkpoint-27000/config.json\n","Model weights saved in test-translation/checkpoint-27000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-27000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-27000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-25500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-27500\n","Configuration saved in test-translation/checkpoint-27500/config.json\n","Model weights saved in test-translation/checkpoint-27500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-27500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-27500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-26000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-28000\n","Configuration saved in test-translation/checkpoint-28000/config.json\n","Model weights saved in test-translation/checkpoint-28000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-28000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-28000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-26500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-28500\n","Configuration saved in test-translation/checkpoint-28500/config.json\n","Model weights saved in test-translation/checkpoint-28500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-28500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-28500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-27000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-29000\n","Configuration saved in test-translation/checkpoint-29000/config.json\n","Model weights saved in test-translation/checkpoint-29000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-29000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-29000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-27500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-29500\n","Configuration saved in test-translation/checkpoint-29500/config.json\n","Model weights saved in test-translation/checkpoint-29500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-29500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-29500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-28000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-30000\n","Configuration saved in test-translation/checkpoint-30000/config.json\n","Model weights saved in test-translation/checkpoint-30000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-30000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-30000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-28500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-30500\n","Configuration saved in test-translation/checkpoint-30500/config.json\n","Model weights saved in test-translation/checkpoint-30500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-30500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-30500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-29000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-31000\n","Configuration saved in test-translation/checkpoint-31000/config.json\n","Model weights saved in test-translation/checkpoint-31000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-31000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-31000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-29500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-31500\n","Configuration saved in test-translation/checkpoint-31500/config.json\n","Model weights saved in test-translation/checkpoint-31500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-31500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-31500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-30000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-32000\n","Configuration saved in test-translation/checkpoint-32000/config.json\n","Model weights saved in test-translation/checkpoint-32000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-32000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-32000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-30500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-32500\n","Configuration saved in test-translation/checkpoint-32500/config.json\n","Model weights saved in test-translation/checkpoint-32500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-32500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-32500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-31000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-33000\n","Configuration saved in test-translation/checkpoint-33000/config.json\n","Model weights saved in test-translation/checkpoint-33000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-33000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-33000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-31500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-33500\n","Configuration saved in test-translation/checkpoint-33500/config.json\n","Model weights saved in test-translation/checkpoint-33500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-33500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-33500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-32000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-34000\n","Configuration saved in test-translation/checkpoint-34000/config.json\n","Model weights saved in test-translation/checkpoint-34000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-34000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-34000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-32500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-34500\n","Configuration saved in test-translation/checkpoint-34500/config.json\n","Model weights saved in test-translation/checkpoint-34500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-34500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-34500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-33000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-35000\n","Configuration saved in test-translation/checkpoint-35000/config.json\n","Model weights saved in test-translation/checkpoint-35000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-35000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-35000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-33500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-35500\n","Configuration saved in test-translation/checkpoint-35500/config.json\n","Model weights saved in test-translation/checkpoint-35500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-35500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-35500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-34000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-36000\n","Configuration saved in test-translation/checkpoint-36000/config.json\n","Model weights saved in test-translation/checkpoint-36000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-36000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-36000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-34500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-36500\n","Configuration saved in test-translation/checkpoint-36500/config.json\n","Model weights saved in test-translation/checkpoint-36500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-36500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-36500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-35000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-37000\n","Configuration saved in test-translation/checkpoint-37000/config.json\n","Model weights saved in test-translation/checkpoint-37000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-37000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-37000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-35500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-37500\n","Configuration saved in test-translation/checkpoint-37500/config.json\n","Model weights saved in test-translation/checkpoint-37500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-37500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-37500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-36000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-38000\n","Configuration saved in test-translation/checkpoint-38000/config.json\n","Model weights saved in test-translation/checkpoint-38000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-38000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-38000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-36500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-38500\n","Configuration saved in test-translation/checkpoint-38500/config.json\n","Model weights saved in test-translation/checkpoint-38500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-38500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-38500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-37000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-39000\n","Configuration saved in test-translation/checkpoint-39000/config.json\n","Model weights saved in test-translation/checkpoint-39000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-39000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-39000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-37500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-39500\n","Configuration saved in test-translation/checkpoint-39500/config.json\n","Model weights saved in test-translation/checkpoint-39500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-39500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-39500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-38000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-40000\n","Configuration saved in test-translation/checkpoint-40000/config.json\n","Model weights saved in test-translation/checkpoint-40000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-40000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-40000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-38500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-40500\n","Configuration saved in test-translation/checkpoint-40500/config.json\n","Model weights saved in test-translation/checkpoint-40500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-40500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-40500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-39000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-41000\n","Configuration saved in test-translation/checkpoint-41000/config.json\n","Model weights saved in test-translation/checkpoint-41000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-41000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-41000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-39500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-41500\n","Configuration saved in test-translation/checkpoint-41500/config.json\n","Model weights saved in test-translation/checkpoint-41500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-41500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-41500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-40000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-42000\n","Configuration saved in test-translation/checkpoint-42000/config.json\n","Model weights saved in test-translation/checkpoint-42000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-42000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-42000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-40500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-42500\n","Configuration saved in test-translation/checkpoint-42500/config.json\n","Model weights saved in test-translation/checkpoint-42500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-42500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-42500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-41000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-43000\n","Configuration saved in test-translation/checkpoint-43000/config.json\n","Model weights saved in test-translation/checkpoint-43000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-43000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-43000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-41500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-43500\n","Configuration saved in test-translation/checkpoint-43500/config.json\n","Model weights saved in test-translation/checkpoint-43500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-43500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-43500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-42000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-44000\n","Configuration saved in test-translation/checkpoint-44000/config.json\n","Model weights saved in test-translation/checkpoint-44000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-44000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-44000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-42500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-44500\n","Configuration saved in test-translation/checkpoint-44500/config.json\n","Model weights saved in test-translation/checkpoint-44500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-44500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-44500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-43000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-45000\n","Configuration saved in test-translation/checkpoint-45000/config.json\n","Model weights saved in test-translation/checkpoint-45000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-45000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-45000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-43500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-45500\n","Configuration saved in test-translation/checkpoint-45500/config.json\n","Model weights saved in test-translation/checkpoint-45500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-45500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-45500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-44000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-46000\n","Configuration saved in test-translation/checkpoint-46000/config.json\n","Model weights saved in test-translation/checkpoint-46000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-46000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-46000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-44500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-46500\n","Configuration saved in test-translation/checkpoint-46500/config.json\n","Model weights saved in test-translation/checkpoint-46500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-46500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-46500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-45000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-47000\n","Configuration saved in test-translation/checkpoint-47000/config.json\n","Model weights saved in test-translation/checkpoint-47000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-47000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-47000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-45500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-47500\n","Configuration saved in test-translation/checkpoint-47500/config.json\n","Model weights saved in test-translation/checkpoint-47500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-47500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-47500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-46000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-48000\n","Configuration saved in test-translation/checkpoint-48000/config.json\n","Model weights saved in test-translation/checkpoint-48000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-48000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-48000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-46500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-48500\n","Configuration saved in test-translation/checkpoint-48500/config.json\n","Model weights saved in test-translation/checkpoint-48500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-48500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-48500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-47000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-49000\n","Configuration saved in test-translation/checkpoint-49000/config.json\n","Model weights saved in test-translation/checkpoint-49000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-49000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-49000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-47500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-49500\n","Configuration saved in test-translation/checkpoint-49500/config.json\n","Model weights saved in test-translation/checkpoint-49500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-49500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-49500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-48000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-50000\n","Configuration saved in test-translation/checkpoint-50000/config.json\n","Model weights saved in test-translation/checkpoint-50000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-50000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-50000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-48500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-50500\n","Configuration saved in test-translation/checkpoint-50500/config.json\n","Model weights saved in test-translation/checkpoint-50500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-50500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-50500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-49000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-51000\n","Configuration saved in test-translation/checkpoint-51000/config.json\n","Model weights saved in test-translation/checkpoint-51000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-51000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-51000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-49500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-51500\n","Configuration saved in test-translation/checkpoint-51500/config.json\n","Model weights saved in test-translation/checkpoint-51500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-51500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-51500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-50000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-52000\n","Configuration saved in test-translation/checkpoint-52000/config.json\n","Model weights saved in test-translation/checkpoint-52000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-52000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-52000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-50500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-52500\n","Configuration saved in test-translation/checkpoint-52500/config.json\n","Model weights saved in test-translation/checkpoint-52500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-52500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-52500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-51000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-53000\n","Configuration saved in test-translation/checkpoint-53000/config.json\n","Model weights saved in test-translation/checkpoint-53000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-53000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-53000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-51500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-53500\n","Configuration saved in test-translation/checkpoint-53500/config.json\n","Model weights saved in test-translation/checkpoint-53500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-53500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-53500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-52000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-54000\n","Configuration saved in test-translation/checkpoint-54000/config.json\n","Model weights saved in test-translation/checkpoint-54000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-54000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-54000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-52500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-54500\n","Configuration saved in test-translation/checkpoint-54500/config.json\n","Model weights saved in test-translation/checkpoint-54500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-54500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-54500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-53000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-55000\n","Configuration saved in test-translation/checkpoint-55000/config.json\n","Model weights saved in test-translation/checkpoint-55000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-55000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-55000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-53500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-55500\n","Configuration saved in test-translation/checkpoint-55500/config.json\n","Model weights saved in test-translation/checkpoint-55500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-55500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-55500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-54000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-56000\n","Configuration saved in test-translation/checkpoint-56000/config.json\n","Model weights saved in test-translation/checkpoint-56000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-56000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-56000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-54500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-56500\n","Configuration saved in test-translation/checkpoint-56500/config.json\n","Model weights saved in test-translation/checkpoint-56500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-56500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-56500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-55000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-57000\n","Configuration saved in test-translation/checkpoint-57000/config.json\n","Model weights saved in test-translation/checkpoint-57000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-57000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-57000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-55500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-57500\n","Configuration saved in test-translation/checkpoint-57500/config.json\n","Model weights saved in test-translation/checkpoint-57500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-57500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-57500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-56000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-58000\n","Configuration saved in test-translation/checkpoint-58000/config.json\n","Model weights saved in test-translation/checkpoint-58000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-58000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-58000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-56500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-58500\n","Configuration saved in test-translation/checkpoint-58500/config.json\n","Model weights saved in test-translation/checkpoint-58500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-58500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-58500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-57000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-59000\n","Configuration saved in test-translation/checkpoint-59000/config.json\n","Model weights saved in test-translation/checkpoint-59000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-59000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-59000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-57500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-59500\n","Configuration saved in test-translation/checkpoint-59500/config.json\n","Model weights saved in test-translation/checkpoint-59500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-59500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-59500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-58000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-60000\n","Configuration saved in test-translation/checkpoint-60000/config.json\n","Model weights saved in test-translation/checkpoint-60000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-60000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-60000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-58500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-60500\n","Configuration saved in test-translation/checkpoint-60500/config.json\n","Model weights saved in test-translation/checkpoint-60500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-60500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-60500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-59000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-61000\n","Configuration saved in test-translation/checkpoint-61000/config.json\n","Model weights saved in test-translation/checkpoint-61000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-61000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-61000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-59500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-61500\n","Configuration saved in test-translation/checkpoint-61500/config.json\n","Model weights saved in test-translation/checkpoint-61500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-61500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-61500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-60000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-62000\n","Configuration saved in test-translation/checkpoint-62000/config.json\n","Model weights saved in test-translation/checkpoint-62000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-62000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-62000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-60500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-62500\n","Configuration saved in test-translation/checkpoint-62500/config.json\n","Model weights saved in test-translation/checkpoint-62500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-62500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-62500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-61000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-63000\n","Configuration saved in test-translation/checkpoint-63000/config.json\n","Model weights saved in test-translation/checkpoint-63000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-63000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-63000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-61500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-63500\n","Configuration saved in test-translation/checkpoint-63500/config.json\n","Model weights saved in test-translation/checkpoint-63500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-63500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-63500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-62000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-64000\n","Configuration saved in test-translation/checkpoint-64000/config.json\n","Model weights saved in test-translation/checkpoint-64000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-64000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-64000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-62500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-64500\n","Configuration saved in test-translation/checkpoint-64500/config.json\n","Model weights saved in test-translation/checkpoint-64500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-64500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-64500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-63000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-65000\n","Configuration saved in test-translation/checkpoint-65000/config.json\n","Model weights saved in test-translation/checkpoint-65000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-65000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-65000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-63500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-65500\n","Configuration saved in test-translation/checkpoint-65500/config.json\n","Model weights saved in test-translation/checkpoint-65500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-65500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-65500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-64000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-66000\n","Configuration saved in test-translation/checkpoint-66000/config.json\n","Model weights saved in test-translation/checkpoint-66000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-66000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-66000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-64500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-66500\n","Configuration saved in test-translation/checkpoint-66500/config.json\n","Model weights saved in test-translation/checkpoint-66500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-66500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-66500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-65000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-67000\n","Configuration saved in test-translation/checkpoint-67000/config.json\n","Model weights saved in test-translation/checkpoint-67000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-67000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-67000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-65500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-67500\n","Configuration saved in test-translation/checkpoint-67500/config.json\n","Model weights saved in test-translation/checkpoint-67500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-67500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-67500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-66000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-68000\n","Configuration saved in test-translation/checkpoint-68000/config.json\n","Model weights saved in test-translation/checkpoint-68000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-68000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-68000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-66500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-68500\n","Configuration saved in test-translation/checkpoint-68500/config.json\n","Model weights saved in test-translation/checkpoint-68500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-68500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-68500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-67000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-69000\n","Configuration saved in test-translation/checkpoint-69000/config.json\n","Model weights saved in test-translation/checkpoint-69000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-69000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-69000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-67500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-69500\n","Configuration saved in test-translation/checkpoint-69500/config.json\n","Model weights saved in test-translation/checkpoint-69500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-69500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-69500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-68000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-70000\n","Configuration saved in test-translation/checkpoint-70000/config.json\n","Model weights saved in test-translation/checkpoint-70000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-70000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-70000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-68500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-70500\n","Configuration saved in test-translation/checkpoint-70500/config.json\n","Model weights saved in test-translation/checkpoint-70500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-70500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-70500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-69000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-71000\n","Configuration saved in test-translation/checkpoint-71000/config.json\n","Model weights saved in test-translation/checkpoint-71000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-71000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-71000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-69500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-71500\n","Configuration saved in test-translation/checkpoint-71500/config.json\n","Model weights saved in test-translation/checkpoint-71500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-71500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-71500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-70000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-72000\n","Configuration saved in test-translation/checkpoint-72000/config.json\n","Model weights saved in test-translation/checkpoint-72000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-72000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-72000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-70500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-72500\n","Configuration saved in test-translation/checkpoint-72500/config.json\n","Model weights saved in test-translation/checkpoint-72500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-72500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-72500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-71000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-73000\n","Configuration saved in test-translation/checkpoint-73000/config.json\n","Model weights saved in test-translation/checkpoint-73000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-73000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-73000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-71500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-73500\n","Configuration saved in test-translation/checkpoint-73500/config.json\n","Model weights saved in test-translation/checkpoint-73500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-73500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-73500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-72000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-74000\n","Configuration saved in test-translation/checkpoint-74000/config.json\n","Model weights saved in test-translation/checkpoint-74000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-74000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-74000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-72500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-74500\n","Configuration saved in test-translation/checkpoint-74500/config.json\n","Model weights saved in test-translation/checkpoint-74500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-74500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-74500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-73000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-75000\n","Configuration saved in test-translation/checkpoint-75000/config.json\n","Model weights saved in test-translation/checkpoint-75000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-75000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-75000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-73500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-75500\n","Configuration saved in test-translation/checkpoint-75500/config.json\n","Model weights saved in test-translation/checkpoint-75500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-75500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-75500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-74000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-76000\n","Configuration saved in test-translation/checkpoint-76000/config.json\n","Model weights saved in test-translation/checkpoint-76000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-76000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-76000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-74500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-76500\n","Configuration saved in test-translation/checkpoint-76500/config.json\n","Model weights saved in test-translation/checkpoint-76500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-76500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-76500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-75000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-77000\n","Configuration saved in test-translation/checkpoint-77000/config.json\n","Model weights saved in test-translation/checkpoint-77000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-77000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-77000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-75500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-77500\n","Configuration saved in test-translation/checkpoint-77500/config.json\n","Model weights saved in test-translation/checkpoint-77500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-77500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-77500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-76000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-78000\n","Configuration saved in test-translation/checkpoint-78000/config.json\n","Model weights saved in test-translation/checkpoint-78000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-78000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-78000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-76500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-78500\n","Configuration saved in test-translation/checkpoint-78500/config.json\n","Model weights saved in test-translation/checkpoint-78500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-78500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-78500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-77000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-79000\n","Configuration saved in test-translation/checkpoint-79000/config.json\n","Model weights saved in test-translation/checkpoint-79000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-79000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-79000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-77500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-79500\n","Configuration saved in test-translation/checkpoint-79500/config.json\n","Model weights saved in test-translation/checkpoint-79500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-79500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-79500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-78000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-80000\n","Configuration saved in test-translation/checkpoint-80000/config.json\n","Model weights saved in test-translation/checkpoint-80000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-80000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-80000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-78500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-80500\n","Configuration saved in test-translation/checkpoint-80500/config.json\n","Model weights saved in test-translation/checkpoint-80500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-80500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-80500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-79000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-81000\n","Configuration saved in test-translation/checkpoint-81000/config.json\n","Model weights saved in test-translation/checkpoint-81000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-81000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-81000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-79500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-81500\n","Configuration saved in test-translation/checkpoint-81500/config.json\n","Model weights saved in test-translation/checkpoint-81500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-81500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-81500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-80000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-82000\n","Configuration saved in test-translation/checkpoint-82000/config.json\n","Model weights saved in test-translation/checkpoint-82000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-82000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-82000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-80500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-82500\n","Configuration saved in test-translation/checkpoint-82500/config.json\n","Model weights saved in test-translation/checkpoint-82500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-82500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-82500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-81000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-83000\n","Configuration saved in test-translation/checkpoint-83000/config.json\n","Model weights saved in test-translation/checkpoint-83000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-83000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-83000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-81500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-83500\n","Configuration saved in test-translation/checkpoint-83500/config.json\n","Model weights saved in test-translation/checkpoint-83500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-83500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-83500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-82000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-84000\n","Configuration saved in test-translation/checkpoint-84000/config.json\n","Model weights saved in test-translation/checkpoint-84000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-84000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-84000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-82500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-84500\n","Configuration saved in test-translation/checkpoint-84500/config.json\n","Model weights saved in test-translation/checkpoint-84500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-84500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-84500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-83000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-85000\n","Configuration saved in test-translation/checkpoint-85000/config.json\n","Model weights saved in test-translation/checkpoint-85000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-85000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-85000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-83500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-85500\n","Configuration saved in test-translation/checkpoint-85500/config.json\n","Model weights saved in test-translation/checkpoint-85500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-85500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-85500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-84000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-86000\n","Configuration saved in test-translation/checkpoint-86000/config.json\n","Model weights saved in test-translation/checkpoint-86000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-86000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-86000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-84500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-86500\n","Configuration saved in test-translation/checkpoint-86500/config.json\n","Model weights saved in test-translation/checkpoint-86500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-86500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-86500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-85000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-87000\n","Configuration saved in test-translation/checkpoint-87000/config.json\n","Model weights saved in test-translation/checkpoint-87000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-87000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-87000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-85500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-87500\n","Configuration saved in test-translation/checkpoint-87500/config.json\n","Model weights saved in test-translation/checkpoint-87500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-87500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-87500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-86000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-88000\n","Configuration saved in test-translation/checkpoint-88000/config.json\n","Model weights saved in test-translation/checkpoint-88000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-88000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-88000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-86500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-88500\n","Configuration saved in test-translation/checkpoint-88500/config.json\n","Model weights saved in test-translation/checkpoint-88500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-88500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-88500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-87000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-89000\n","Configuration saved in test-translation/checkpoint-89000/config.json\n","Model weights saved in test-translation/checkpoint-89000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-89000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-89000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-87500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-89500\n","Configuration saved in test-translation/checkpoint-89500/config.json\n","Model weights saved in test-translation/checkpoint-89500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-89500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-89500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-88000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-90000\n","Configuration saved in test-translation/checkpoint-90000/config.json\n","Model weights saved in test-translation/checkpoint-90000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-90000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-90000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-88500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","Saving model checkpoint to test-translation/checkpoint-90500\n","Configuration saved in test-translation/checkpoint-90500/config.json\n","Model weights saved in test-translation/checkpoint-90500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-90500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-90500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-89000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-91000\n","Configuration saved in test-translation/checkpoint-91000/config.json\n","Model weights saved in test-translation/checkpoint-91000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-91000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-91000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-89500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-91500\n","Configuration saved in test-translation/checkpoint-91500/config.json\n","Model weights saved in test-translation/checkpoint-91500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-91500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-91500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-90000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-92000\n","Configuration saved in test-translation/checkpoint-92000/config.json\n","Model weights saved in test-translation/checkpoint-92000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-92000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-92000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-90500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-92500\n","Configuration saved in test-translation/checkpoint-92500/config.json\n","Model weights saved in test-translation/checkpoint-92500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-92500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-92500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-91000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-93000\n","Configuration saved in test-translation/checkpoint-93000/config.json\n","Model weights saved in test-translation/checkpoint-93000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-93000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-93000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-91500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-93500\n","Configuration saved in test-translation/checkpoint-93500/config.json\n","Model weights saved in test-translation/checkpoint-93500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-93500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-93500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-92000] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-94000\n","Configuration saved in test-translation/checkpoint-94000/config.json\n","Model weights saved in test-translation/checkpoint-94000/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-94000/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-94000/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-92500] due to args.save_total_limit\n","Saving model checkpoint to test-translation/checkpoint-94500\n","Configuration saved in test-translation/checkpoint-94500/config.json\n","Model weights saved in test-translation/checkpoint-94500/pytorch_model.bin\n","tokenizer config file saved in test-translation/checkpoint-94500/tokenizer_config.json\n","Special tokens file saved in test-translation/checkpoint-94500/special_tokens_map.json\n","Deleting older checkpoint [test-translation/checkpoint-93000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1299: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n","  args.max_grad_norm,\n","The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n","***** Running Evaluation *****\n","  Num examples = 2818\n","  Batch size = 16\n","/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n","To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n","  return torch.floor_divide(self, other)\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=94761, training_loss=2.42046151580007, metrics={'train_runtime': 16412.0065, 'train_samples_per_second': 92.381, 'train_steps_per_second': 5.774, 'total_flos': 1.05593737700352e+17, 'train_loss': 2.42046151580007, 'epoch': 1.0})"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"QbwolHe318Hz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625417940554,"user_tz":-330,"elapsed":1950,"user":{"displayName":"Siddartha Chennareddy","photoUrl":"","userId":"05578587641916586628"}},"outputId":"135844ff-8a23-429b-9a6e-4531074dd602"},"source":["model.save_pretrained(\"/content/drive/My Drive/News article summarizer/Ru_En\")\n","tokenizer.save_pretrained(\"/content/drive/My Drive/News article summarizer/Ru_En\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Configuration saved in /content/drive/My Drive/News article summarizer/Ru_En/config.json\n","Model weights saved in /content/drive/My Drive/News article summarizer/Ru_En/pytorch_model.bin\n","tokenizer config file saved in /content/drive/My Drive/News article summarizer/Ru_En/tokenizer_config.json\n","Special tokens file saved in /content/drive/My Drive/News article summarizer/Ru_En/special_tokens_map.json\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["('/content/drive/My Drive/News article summarizer/Ru_En/tokenizer_config.json',\n"," '/content/drive/My Drive/News article summarizer/Ru_En/special_tokens_map.json',\n"," PosixPath('/content/drive/My Drive/News article summarizer/Ru_En/source_spm'),\n"," PosixPath('/content/drive/My Drive/News article summarizer/Ru_En/target_spm'),\n"," PosixPath('/content/drive/My Drive/News article summarizer/Ru_En/vocab'),\n"," PosixPath('/content/drive/My Drive/News article summarizer/Ru_En/tokenizer_config_file'),\n"," '/content/drive/My Drive/News article summarizer/Ru_En/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":4}]}]}